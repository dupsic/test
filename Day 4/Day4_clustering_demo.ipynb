{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CLUSTER ANALYSIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import random\n",
    "\n",
    "random.seed(123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s look into one of the most common clustering algorithms: K-Means in detail. Let’s see a simple example of how K-Means clustering can be used to segregate the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we'll use the `make_blobs` the command to generate isotropic gaussian blobs which can be used for clustering.\n",
    "We specify the number of samples to be generated to be 100 and the number of centers to be 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of clusters. make a variable as we may want to change it later\n",
    "k = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "X, y = make_blobs(n_samples=100, n_features=2, centers=k, random_state=555)\n",
    "X, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X[:, 0], X[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X[:, 0], X[:, 1], c=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see from the above graph, there are 5 clusters that can be created from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you look at the value of y, you can see that the points are classified based on their clusters that are predefined by using make_blobs command, we will be using this only for evaluating purpose.\n",
    "\n",
    "For using K-Means you need to import KMeans from sklearn.cluster library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For using KMeans, you need to specify a number of clusters as arguments. As we set k=5 at the beginning, we will continue with this number and see the results of step by step algorithm, and compare various methods.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Cluster = KMeans(n_clusters=k, max_iter=200)\n",
    "Cluster.fit(X)\n",
    "y_pred = Cluster.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After passing the arguments, we fit the model and predict the results. Now let’s visualize our predictions in a scatter plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X[:, 0], X[:, 1], c=y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that the predicted clusters are mostly the same as the clusters that you saw in the initial scatter plot. Now let’s look into how exactly the K-Means algorithm works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Steps Involved\n",
    "There are 3 important steps in K-Means Clustering.\n",
    "\n",
    "1. Initialize centroids – This is done by randomly choosing K no of points, the points can be present in the dataset or also random points.\n",
    "2. Assign Clusters – The clusters are assigned to each point in the dataset by calculating their distance from the centroid and assigning it to the clustersroid with minimum distance.\n",
    "3. Re-calculate the centroids – Updating the centroid by calculating the centroid of each cluster we have created.\n",
    "Let’s look into this by an example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use the same `make_blobs` example we used at the beginning.\n",
    "\n",
    "We will try to do the clustering without using the KMeans library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting the number of training examples\n",
    "m=X.shape[0]\n",
    "n=X.shape[1] \n",
    "n_iter=200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will set the K value to be 5 as before and also initialize the centroids randomly using the `random.randint()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating an empty centroid array\n",
    "centroids=np.array([]).reshape(n,0) \n",
    "\n",
    "# creating 5 random centroids\n",
    "for i in range(k):\n",
    "    centroids=np.c_[centroids,X[random.randint(0,m-1)]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X[:,0],X[:,1])\n",
    "plt.scatter(centroids[0,:],centroids[1,:],s=300,c='yellow')\n",
    "plt.rcParams.update({'figure.figsize':(10,7.5), 'figure.dpi':100})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll find the distance between the points. Euclidean distance is most commonly used for finding the similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output={}\n",
    "\n",
    "# creating an empty array\n",
    "euclid=np.array([]).reshape(m,0)\n",
    "\n",
    "# finding distance between for each centroid\n",
    "for i in range(k):\n",
    "       dist=np.sum((X-centroids[:,i])**2,axis=1)\n",
    "       euclid=np.c_[euclid,dist]\n",
    "\n",
    "# storing the minimum value we have computed\n",
    "new_clusters=np.argmin(euclid,axis=1)+1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each row represents a data point <br>\n",
    "Each column represents the distance to a specific centroid<br>\n",
    "This allows the algorithm to later find which centroid is closest to each point by taking np.argmin(euclid, axis=1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "euclid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we regroup the dataset based on the minimum values we got and calculate the new centroid value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# computing the mean of separated clusters\n",
    "clusters={}\n",
    "# creating an empty array\n",
    "for i in range(k):\n",
    "    clusters[i+1]=np.array([]).reshape(2,0)\n",
    "\n",
    "# assigning clusters to the points\n",
    "for i in range(m):\n",
    "    clusters[new_clusters[i]]=np.c_[clusters[new_clusters[i]],X[i]]\n",
    "for i in range(k):\n",
    "    clusters[i+1]=clusters[i+1].T\n",
    "# print(clusters)\n",
    "\n",
    "# computing mean and updating it\n",
    "for i in range(k):\n",
    "     centroids[:,i]=np.mean(clusters[i+1],axis=0)\n",
    "centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(k):\n",
    "    plt.scatter(clusters[i+1][:,0],clusters[i+1][:,1])\n",
    "plt.scatter(centroids[0,:],centroids[1,:],s=300,c='yellow')\n",
    "plt.rcParams.update({'figure.figsize':(10,7.5), 'figure.dpi':100})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we need to repeat the above 2 steps over and over again until we reach the convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# repeating the above steps again and again\n",
    "for i in range(n_iter):\n",
    "      euclid=np.array([]).reshape(m,0)\n",
    "      for i in range(k):\n",
    "          dist=np.sum((X-centroids[:,i])**2,axis=1)\n",
    "          euclid=np.c_[euclid,dist]\n",
    "      C=np.argmin(euclid,axis=1)+1\n",
    "      clusters={}\n",
    "      for i in range(k):\n",
    "           clusters[i+1]=np.array([]).reshape(2,0)\n",
    "      for i in range(m):\n",
    "           clusters[C[i]]=np.c_[clusters[C[i]],X[i]]\n",
    "      for i in range(k):\n",
    "           clusters[i+1]=clusters[i+1].T\n",
    "      for i in range(k):\n",
    "           centroids[:,i]=np.mean(clusters[i+1],axis=0)\n",
    "      final=clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s plot it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X[:,0],X[:,1])\n",
    "plt.title('Original Dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the original dataset where it is hard to visually distinguish the clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(k):\n",
    "    plt.scatter(final[i+1][:,0],final[i+1][:,1])\n",
    "plt.scatter(centroids[0,:],centroids[1,:],s=300,c='yellow')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the plot obtained by built-in KMeans \n",
    "plt.scatter(X[:, 0], X[:, 1], c=y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elbow method to find the optimal number of clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the important steps in K-Means Clustering is to determine the optimal number of clusters we need to give as an input. This can be done by iterating it through a number of n values (clusters) and then finding the optimal n value.\n",
    "\n",
    "For finding this optimal n, the Elbow Method is used.\n",
    "\n",
    "You have to plot the loss values vs the n value and find the point where the graph is flattening, this point is considered as the optimal n value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s look at the example we have seen at first, to see the working of the elbow method.\n",
    "\n",
    "We will to iterate it through a series of n values ranging from 1-20 and then plot their loss values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elbow=[]\n",
    "for i in range(2, 21):  # we don't need 0 and 1\n",
    "    kmeans = KMeans(n_clusters=i, init='k-means++', random_state=555, max_iter=200)\n",
    "    kmeans.fit(X)\n",
    "    elbow.append(kmeans.inertia_)\n",
    "\n",
    "plt.plot(range(2, 21), elbow)\n",
    "plt.title('ELBOW METHOD')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that the graph starts to flatten after reaching 5, which means that even if we increase the no of clusters after that point, there is no significant change in the loss value. So we can take the optimal value to be 5 which we also confirmed by visualizing the scatter plot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Average Silhouette Method to find the optimal number of clusters\n",
    "\n",
    "The silhouette coefficient is a measure of cluster cohesion and separation. It quantifies how well a data point fits into its assigned cluster based on two factors:\n",
    "\n",
    "* How close the data point is to other points in the cluster\n",
    "* How far away the data point is from points in other clusters\n",
    "Silhouette coefficient values range between -1 and 1. Larger numbers indicate that samples are closer to their clusters than they are to other clusters.\n",
    "\n",
    "In the `scikit-learn` implementation of the silhouette coefficient, the average silhouette coefficient of all the samples is summarized into one score. The silhouette `score()` function needs a minimum of two clusters, or it will raise an exception.\n",
    "\n",
    "Loop through values of `k` again. This time compute the silhouette coefficient:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score\n",
    "silhouette_coefficients = []\n",
    "\n",
    "# Notice you start at 2 clusters for silhouette coefficient\n",
    "for i in range(2, 20):\n",
    "    kmeans = KMeans(n_clusters=i, init='k-means++', random_state=555, max_iter=200)\n",
    "    kmeans.fit(X)\n",
    "    score = silhouette_score(X, kmeans.labels_)\n",
    "    silhouette_coefficients.append(score)\n",
    "    \n",
    "# silhouette_coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(2, 20), silhouette_coefficients)\n",
    "plt.xticks(range(2, 20))\n",
    "plt.xlabel(\"Number of Clusters\")\n",
    "plt.ylabel(\"Silhouette Coefficient\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ultimately, your decision on the number of clusters to use should be guided by a combination of domain knowledge and clustering evaluation metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some other examples of built in functions and clustering alghoritms. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hierarchical Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.cluster.hierarchy as shc\n",
    "plt.figure(figsize=(10, 7))  \n",
    "plt.title(\"Dendrograms\")  \n",
    "dend = shc.dendrogram(shc.linkage(X, method='ward'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The x-axis represents our data points and y-axis - the Euclidean distance between the clusters. How to determine the optimal number of clusters from this diagram? Just visually, we can look for the largest distance (y axis) that we can vertically, without crossing any horizontal line (before splitting). The longer is the distance, the more clusters differ from each other. \n",
    "In this example, we can imagine a horizontal line above ~15. Since the distances between the corresponding clusters are rather small, we can safely draw a line at position 20 to determine the optimal number of clusters. In our case, the optimal number of clusters is between 4. This also corresponds to the elbow method, but differs from silouette coefficient method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting hierarchical clustering to the dataset\n",
    "# There are two algorithms for hierarchical clustering: Agglomerative Hierarchical Clustering and \n",
    "# Divisive Hierarchical Clustering. We choose Euclidean distance and ward method for our algorithm class\n",
    "\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "hc = AgglomerativeClustering(n_clusters=4, metric='euclidean', linkage='ward')\n",
    "y_hc = hc.fit_predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(4):\n",
    "    plt.scatter(X[y_hc == i,0],X[y_hc == i,1])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DBSCAN (Density-Based Spatial Clustering of Applications with Noise)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform DBSCAN clustering from vector array or distance matrix.\n",
    "\n",
    "DBSCAN - Density-Based Spatial Clustering of Applications with Noise. Finds core samples of high density and expands clusters from them. Good for data which contains clusters of similar density (https://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's first display again our original data\n",
    "plt.scatter(X[:, 0], X[:, 1])\n",
    "# setting y-axis limits for better evaluation of the distance between the points\n",
    "plt.ylim((-12,12))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will construct a DBSCAN object that requires a minimum of 4 data points in a neighborhood of radius 2.5 to be considered a core point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "# define the model\n",
    "model = DBSCAN(eps=2.5, min_samples=4)\n",
    "# fit model and predict clusters\n",
    "yhat = model.fit_predict(X)\n",
    "# retrieve unique clusters\n",
    "clusters = np.unique(yhat)\n",
    "clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create scatter plot for samples from each cluster\n",
    "for cluster in clusters:\n",
    "    # get row indexes for samples with this cluster\n",
    "    row_ix = np.where(yhat == cluster)\n",
    "    # create scatter of these samples\n",
    "    plt.scatter(X[row_ix, 0], X[row_ix, 1])\n",
    "    plt.ylim((-12,12))\n",
    "    \n",
    "# show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Principal Component Analysis (PCA)\n",
    "\n",
    "PCA is a dimensionality reduction technique that transforms high-dimensional data into a lower-dimensional space while preserving as much variance as possible. It's useful for:\n",
    "* Visualizing high-dimensional data\n",
    "* Reducing computational complexity\n",
    "* Removing noise and redundant features\n",
    "* Data compression\n",
    "\n",
    "PCA works by finding the principal components (directions of maximum variance) in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1: PCA on the Iris Dataset\n",
    "\n",
    "Let's start with a classic example using the Iris dataset, which has 4 features. We'll reduce it to 2 dimensions for visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "X_iris = iris.data\n",
    "y_iris = iris.target\n",
    "\n",
    "print(f\"Original shape: {X_iris.shape}\")\n",
    "print(f\"Features: {iris.feature_names}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standardize the features before applying PCA (important when features have different scales):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_iris_scaled = scaler.fit_transform(X_iris)\n",
    "\n",
    "# Apply PCA to reduce to 2 components\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X_iris_scaled)\n",
    "\n",
    "print(f\"Shape after PCA: {X_pca.shape}\")\n",
    "print(f\"\\nExplained variance ratio: {pca.explained_variance_ratio_}\")\n",
    "print(f\"Total variance explained: {pca.explained_variance_ratio_.sum():.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pca"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the data in 2D PCA space:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a scatter plot\n",
    "plt.figure(figsize=(10, 7))\n",
    "scatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y_iris, cmap='viridis', s=50, alpha=0.7)\n",
    "plt.xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.2%} variance)')\n",
    "plt.ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.2%} variance)')\n",
    "plt.title('PCA of Iris Dataset')\n",
    "plt.colorbar(scatter, label='Species')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The PCA plot of the Iris dataset shows how the 4-dimensional flower measurements are transformed into a 2-dimensional space. Here's what each element means:\n",
    "\n",
    "Axes:\n",
    "\n",
    "PC1 (X-axis): The first principal component - the direction in the original 4D space that captures the maximum variance in the data. The percentage shows how much of the total variance this component explains (typically ~73%).\n",
    "PC2 (Y-axis): The second principal component - the direction orthogonal (perpendicular) to PC1 that captures the second-most variance (typically ~23%).\n",
    "Colors: Each color represents one of the three Iris species:<br>\n",
    "\n",
    "Setosa (one cluster)<br>\n",
    "Versicolor (middle cluster)<br>\n",
    "Virginica (another cluster)<br>\n",
    "What it reveals:\n",
    "\n",
    "Dimensionality Reduction: The original 4 features (sepal length, sepal width, petal length, petal width) are compressed into just 2 dimensions while retaining ~95% of the variance.\n",
    "\n",
    "Species Separation: The plot clearly shows that:\n",
    "\n",
    "One species (typically Setosa) is very distinct and well-separated\n",
    "The other two species (Versicolor and Virginica) have some overlap but are mostly distinguishable\n",
    "Clustering Structure: Even without knowing the species labels, you can visually identify 3 distinct groups, which validates that clustering algorithms would work well on this data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determining the Optimal Number of Components\n",
    "\n",
    "Use the explained variance to decide how many components to keep:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply PCA with all components\n",
    "pca_full = PCA()\n",
    "pca_full.fit(X_iris_scaled)\n",
    "\n",
    "# Plot cumulative explained variance\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(range(1, len(pca_full.explained_variance_ratio_) + 1), \n",
    "         pca_full.explained_variance_ratio_, 'bo-')\n",
    "plt.xlabel('Principal Component')\n",
    "plt.ylabel('Explained Variance Ratio')\n",
    "plt.title('Scree Plot')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(range(1, len(pca_full.explained_variance_ratio_) + 1),\n",
    "         np.cumsum(pca_full.explained_variance_ratio_), 'ro-')\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Cumulative Explained Variance')\n",
    "plt.title('Cumulative Explained Variance')\n",
    "plt.axhline(y=0.95, color='k', linestyle='--', label='95% threshold')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Explained variance by each component:\")\n",
    "for i, var in enumerate(pca_full.explained_variance_ratio_):\n",
    "    print(f\"PC{i+1}: {var:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing Principal Components\n",
    "\n",
    "Let's examine what the principal components actually represent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Component loadings (how much each original feature contributes to each PC)\n",
    "components_df = pd.DataFrame(\n",
    "    pca.components_.T,\n",
    "    columns=['PC1', 'PC2'],\n",
    "    index=iris.feature_names\n",
    ")\n",
    "\n",
    "print(\"Component loadings (feature contributions to PCs):\")\n",
    "print(components_df)\n",
    "\n",
    "# Visualize as heatmap\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.heatmap(components_df, annot=True, cmap='coolwarm', center=0, \n",
    "            cbar_kws={'label': 'Loading'}, fmt='.3f')\n",
    "plt.title('PCA Component Loadings\\n(How features contribute to principal components)')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using PCA with a Classifier to Predict Species\n",
    "\n",
    "You can use PCA to reduce dimensionality, then train a classifier (e.g., Logistic Regression) on the PCA-transformed data. Here is an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "species_names = iris.target_names\n",
    "colors = ['purple', 'green', 'yellow']\n",
    "for i, name in enumerate(species_names):\n",
    "    plt.scatter(X_pca[y_iris == i, 0], X_pca[y_iris == i, 1], \n",
    "                label=name, color=colors[i], s=50, alpha=0.7)\n",
    "plt.xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.2%} variance)')\n",
    "plt.ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.2%} variance)')\n",
    "plt.title('PCA of Iris Dataset')\n",
    "plt.legend(title='Species')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Use the PCA-transformed data (X_pca) and true labels (y_iris)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_pca, y_iris, test_size=0.3, random_state=101)\n",
    "\n",
    "# Train a classifier on the PCA data\n",
    "clf = LogisticRegression(max_iter=200)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Evaluate performance\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred, target_names=iris.target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the original data (X_iris) and true labels (y_iris)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_iris, y_iris, test_size=0.3, random_state=101)\n",
    "\n",
    "# Train a classifier on the original data\n",
    "clf = LogisticRegression(max_iter=200)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Evaluate performance\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred, target_names=iris.target_names))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bootcamp2026 (3.14.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
