{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ybyz0S6sQj_T"
   },
   "source": [
    "# Information Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "brzA9z-U_TBc"
   },
   "source": [
    "All assignments were tested in the [Google Colab](https://colab.research.google.com/notebooks/). All core packages are already installed there. \n",
    "\n",
    "If you use your python environment, probably you'll need to install them separately via pip or conda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EQjFTtuFFhr1"
   },
   "source": [
    "## Part I. Regular Expressions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eBstqhbBEmui"
   },
   "source": [
    "### 1.1 Basic Regex\n",
    "Write a regular expression to search for emails in text.\n",
    "\n",
    "Let's start from the basic emails like john@gmail.com\n",
    "\n",
    "<details>\n",
    "<summary>hints:</summary>\n",
    "\n",
    "you need to catch patterns like word@word.word\n",
    "\n",
    "one of the ways to catch any word character is **\\w** \n",
    "\n",
    "if you need to repeat some character you can use **+**\n",
    "\n",
    "so to catch the full word **\\w+** will be enough\n",
    "\n",
    "if you need to escape some character (because it's a reserved character), you can use **\\\\**, e.g. **\\\\.**\n",
    "\n",
    "you can find all special characters in the [documentation](https://docs.python.org/3/library/re.html)\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "E76zlryWCux9"
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1219793315.py, line 5)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mpattern = # PUT YOUR CODE HERE\u001b[39m\n              ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def catch_simple_emails(text: str) -> list:\n",
    "\n",
    "  pattern = # PUT YOUR CODE HERE\n",
    "  email_regex = re.compile(pattern, re.I)       \n",
    "  emails = email_regex.findall(txt)\n",
    "  return emails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "yrqrn4sEE4Xf"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'catch_simple_emails' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m txt = \u001b[33m\"\u001b[39m\u001b[33mMost beginners use generic free business email accounts without a domain name which isn’t very professional. For example: johnsmith2019@gmail.com or jsmithfromstargardening@yahoo.com\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m   \u001b[38;5;28;01massert\u001b[39;00m \u001b[43mcatch_simple_emails\u001b[49m(txt) == [\u001b[33m'\u001b[39m\u001b[33mjohnsmith2019@gmail.com\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mjsmithfromstargardening@yahoo.com\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m      4\u001b[39m   \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[33mpassed\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m:\n",
      "\u001b[31mNameError\u001b[39m: name 'catch_simple_emails' is not defined"
     ]
    }
   ],
   "source": [
    "txt = \"Most beginners use generic free business email accounts without a domain name which isn’t very professional. For example: johnsmith2019@gmail.com or jsmithfromstargardening@yahoo.com\"\n",
    "try:\n",
    "  assert catch_simple_emails(txt) == ['johnsmith2019@gmail.com', 'jsmithfromstargardening@yahoo.com']\n",
    "  print('passed')\n",
    "except AssertionError:\n",
    "  print('failed')\n",
    "  print(catch_simple_emails(txt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "alm-dBYDK4gM"
   },
   "source": [
    "### 1.2 Complex Regex\n",
    "How can it be modified to catch more complex emails like john.smith@yahoo.co.id or john-smith@yahoo.co.id?\n",
    "\n",
    "Emails also can contain other symbols but they are not required in this assignment\n",
    "<details>\n",
    "<summary>hints:</summary>\n",
    "\n",
    "you need to replace single words with word sequences separated by dots or hyphens\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VvueiNKuFOXF"
   },
   "outputs": [],
   "source": [
    "def catch_complex_emails(txt: str) -> list:\n",
    "  pattern = # PUT YOUR CODE HERE\n",
    "  email_regex = re.compile(pattern, re.I)       \n",
    "  emails = email_regex.findall(txt)\n",
    "  return emails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M6o61KhrGfU_"
   },
   "outputs": [],
   "source": [
    "txt = \"\"\"\n",
    "simple@example.com nuff said\n",
    "very.common@example.com very.very.common\n",
    "very.common@long.example.com multiple domain levels\n",
    "other.email-with-hyphen@example.com dot + hyphen\n",
    "fully-qualified-domain@example.com hyphen only\n",
    "x@example.com single letter\n",
    "example-indeed@strange-example.com one more example\n",
    "\"\"\"\n",
    "try:\n",
    "  assert catch_complex_emails(txt) == ['simple@example.com',\n",
    "  'very.common@example.com',\n",
    "  'very.common@long.example.com',\n",
    "  'other.email-with-hyphen@example.com',\n",
    "  'fully-qualified-domain@example.com',\n",
    "  'x@example.com',\n",
    "  'example-indeed@strange-example.com']\n",
    "  print('passed')\n",
    "except AssertionError:\n",
    "  print('failed')\n",
    "  print(catch_complex_emails(txt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WBsP3w3dWArL"
   },
   "source": [
    "### 1.3 Regex-based Tokenizer\n",
    "\n",
    "Using your knowledge of regular expressions, write a simple text tokenizer that can separate punctuation, numbers, and words into individual tokens.\n",
    "\n",
    "Assumptions:\n",
    "\n",
    "* Floating point numbers like 1.23 fit in one token. The decimal separator can be either a dot or a comma.\n",
    "* The number can be negative and have a sign like -10.5.\n",
    "* There may not be an integer part of the number at all: the sequences -0.15 and -.15 mean the same number.\n",
    "* Empty fractional part is not allowed: string \"10.\" must be split into two tokens: \"10\" and \".\"\n",
    "* Consecutive punctuation marks are separated each into a separate token.\n",
    "* Only English letters are allowed in words.\n",
    "* Empty tokens must be removed\n",
    "\n",
    "<details>\n",
    "<summary>hints:</summary>\n",
    "\n",
    "To combine individual checks into a single regex, you can use the operator | (or), e.g. '[a-z]|[0-9]' means one letter from a to z or one number from 0 to 9\n",
    "\n",
    "Remember that if regex contains multiple blocks, they are processed in natural order (from left to right), so you should catch more complex elements first (e.g., catch floating point numbers before integers); otherwise, you can lose it.\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J5YCRx8bkbq2"
   },
   "outputs": [],
   "source": [
    "def text_split(txt: str) -> list:\n",
    "  pattern = # PUT YOUR CODE HERE\n",
    "  token_regex = re.compile(pattern, re.I)       \n",
    "  tokens = token_regex.findall(txt)\n",
    "  return tokens  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ymJSyHPblaGE"
   },
   "outputs": [],
   "source": [
    "txt = {\n",
    "  '': [],\n",
    "  '$0.00 \"Surplus\";$-0.00 \"Shortage\"': ['$', '0.00', '\"', 'Surplus', '\"', ';', '$', '-0.00', '\"', 'Shortage', '\"'],\n",
    "  'Combine text and numbers': ['Combine', 'text', 'and', 'numbers'],\n",
    "  'Combine_text_and$$numbers': ['Combine', '_', 'text', '_', 'and', '$', '$', 'numbers'],\n",
    "  '(.4 in this example)': ['(', '.4', 'in', 'this', 'example', ')'],\n",
    "  '(4. in this example)': ['(', '4', '.', 'in', 'this', 'example', ')'],\n",
    "  '$$1,1%#-2': ['$', '$', '1,1', '%', '#', '-2'],\n",
    "  '...': ['.', '.', '.'],\n",
    "  'How to Sauté?': ['How', 'to', 'Saut', 'é', '?']\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JzT6dxu8l9Ty"
   },
   "outputs": [],
   "source": [
    "for x in txt:\n",
    "  try:\n",
    "    assert text_split(x) == txt[x]\n",
    "    print('passed')\n",
    "  except AssertionError:\n",
    "    print('failed')\n",
    "    print(text_split(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sYD-unDcs55m"
   },
   "source": [
    "## Part II. Fuzzy matching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GkfWi9NRDz13"
   },
   "source": [
    "Fuzzy Matching (also called Approximate String Matching) is a technique that helps identify two elements of text, strings, or entries that are approximately similar but are not exactly the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EISOAKlEtPZ5"
   },
   "source": [
    "### 2.1 Levenshtein distance\n",
    "\n",
    "The Levenshtein distance is a metric used to measure the difference between 2 string sequences.\n",
    "\n",
    "It gives us a measure of the number of single character insertions, deletions or substitutions required to change one string into another.\n",
    "\n",
    " You can find an algorythm and the full description [here](https://en.wikipedia.org/wiki/Levenshtein_distance)\n",
    "\n",
    "For our task we'll use python-levenshtein library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WIPv-N2yGh22"
   },
   "outputs": [],
   "source": [
    "!pip install python-levenshtein # if not already installed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KL4TrXgWtS-i"
   },
   "outputs": [],
   "source": [
    "# simple approach\n",
    "import string\n",
    "from Levenshtein import distance as levenshtein_distance\n",
    "\n",
    "txt = 'Health chiefs today announced a fresh Covid vaccine programme ahead of what they fear will be a difficult winter.'\n",
    "clean_txt = ''.join([c for c in txt.lower() if c not in string.punctuation])\n",
    "words = [i for i in clean_txt.split()]\n",
    "pattern = 'Covid'\n",
    "\n",
    "for word in words:\n",
    "  print(f\"distance between {word} and {pattern} is {levenshtein_distance(word, pattern)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "24vG07zHIhR_"
   },
   "source": [
    "Your task.\n",
    "Implement a function that will return the word from the text that is most similar to the given pattern.\n",
    "\n",
    "If there are many words with the same distance, please use the first one\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CE6Kf4GyGpg8"
   },
   "outputs": [],
   "source": [
    "def find_similar_word(txt: str, pattern: str) -> str:\n",
    "\n",
    "  out = ''\n",
    "  # PUT YOUR CODE HERE\n",
    "  \n",
    "  return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "US6hul6lGpkG"
   },
   "outputs": [],
   "source": [
    "txt = 'spaCy is a free open-source library for Natural Language Processing in Python. It features NER, POS tagging, dependency parsing, word vectors and more.'\n",
    "try:\n",
    "  assert find_similar_word(txt, 'dependent') == 'dependency'\n",
    "  print('passed')\n",
    "except AssertionError:\n",
    "  print('failed')\n",
    "  print(find_similar_word(txt, 'dependent'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9w5sIfmFTxvU"
   },
   "source": [
    "Implement a function that will return the pair of most similar words in the given text.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7J3ibh4uUjAA"
   },
   "outputs": [],
   "source": [
    "def find_similar_words(txt: str) -> list: # output should be like ['word1', 'word2']\n",
    "\n",
    "  out = []\n",
    "  # PUT YOUR CODE HERE\n",
    "  \n",
    "  return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5X0P-UqiVia1"
   },
   "outputs": [],
   "source": [
    "txt = 'Sentence splitting is the process of dividing text into sentences'\n",
    "try:\n",
    "  assert find_similar_words(txt) == ['sentence', 'sentences']\n",
    "  print('passed')\n",
    "except AssertionError:\n",
    "  print('failed')\n",
    "  print(find_similar_words(txt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "An8ReNtZbU9B"
   },
   "source": [
    "You can find an additional fuzzy matching component for SpaCy here:\n",
    "https://github.com/gandersen101/spaczz\n",
    "\n",
    "There are many different libraries for this, such as tfidf-matcher, fuzzywuzzy, rapidfuzz, etc.\n",
    "\n",
    "For simplicity, we will not touch them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6A8htU4Vyrv4"
   },
   "source": [
    "## Part III. Named Entity Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ArQl_x1lzbJp"
   },
   "source": [
    "Named-entity recognition (NER) (also known as (named) entity identification, entity chunking, and entity extraction) is a subtask of information extraction that seeks to locate and classify named entities mentioned in unstructured text into pre-defined categories such as person names, organizations, locations, medical codes, time expressions, quantities, monetary values, percentages, etc.\n",
    "\n",
    "For this task it is better to use either **NLTK** or **SpaCy** libraries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iOe2NSso-_2e"
   },
   "source": [
    "### 3.1 NLTK Approach "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qGGNZmfb0OF7"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "nltk.download('maxent_ne_chunker_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cfdsfLWy0Rlt"
   },
   "outputs": [],
   "source": [
    "def nltk_ner(txt: str) -> list:\n",
    "\n",
    "  out = []\n",
    "\n",
    "  for sent in nltk.sent_tokenize(txt):\n",
    "    for chunk in nltk.ne_chunk(nltk.pos_tag(nltk.word_tokenize(sent))):\n",
    "      if hasattr(chunk, 'label'):\n",
    "          out.append(f\"{chunk.label()} {' '.join(c[0] for c in chunk)}\")\n",
    "  return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P48ljJYl3Lw0"
   },
   "outputs": [],
   "source": [
    "txt = \"Boris Johnson skipped out on his second emergency meeting in a row today and tomorrow he is hosting a lavish goodbye bash at Chequers. A spokesman said Sunday's party was a 'private event'.\"\n",
    "nltk_ner(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iTFdEgo8WQbv"
   },
   "outputs": [],
   "source": [
    "# !pip install svgling\n",
    "nltk.ne_chunk(nltk.pos_tag(nltk.word_tokenize(nltk.sent_tokenize(txt)[0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k80aW_vC2lKG"
   },
   "source": [
    "### 3.2 SpaCy Approach\n",
    "Your task.\n",
    "\n",
    "Repeat the same procedure using the Spacy library. Compare the results of both functions. What can you say about it?\n",
    "\n",
    "list of all useful properties:  [Doc](https://spacy.io/api/doc#ents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k7ZuIX2b0OsT"
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0jZNLxNQ4Juz"
   },
   "outputs": [],
   "source": [
    "def spacy_ner(txt: str) -> list: # output should be like ['label1 word1', 'label2 word2', ...]\n",
    "  doc = nlp(txt)\n",
    "  out = []\n",
    "  # PUT YOUR CODE HERE\n",
    "  return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Sib8gDjk0skw"
   },
   "outputs": [],
   "source": [
    "txt = \"Boris Johnson skipped out on his second emergency meeting in a row today and tomorrow he is hosting a lavish goodbye bash at Chequers. A spokesman said Sunday's party was a 'private event'.\"\n",
    "try:\n",
    "  assert spacy_ner(txt) == ['PERSON Boris Johnson', 'ORDINAL second', 'DATE today', 'DATE tomorrow', 'DATE Sunday']\n",
    "  print('passed')\n",
    "except AssertionError:\n",
    "  print('failed')\n",
    "  print(spacy_ner(txt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZZpX0e4U5ucI"
   },
   "source": [
    "SpaCy can also render named entities so you can check yourself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gCUQQzcB7R7K"
   },
   "outputs": [],
   "source": [
    "from spacy import displacy  \n",
    "displacy.render(nlp(txt), jupyter=True, style='ent')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "eBstqhbBEmui",
    "alm-dBYDK4gM",
    "WBsP3w3dWArL",
    "EISOAKlEtPZ5",
    "iOe2NSso-_2e",
    "k80aW_vC2lKG"
   ],
   "name": "Information Extraction.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
