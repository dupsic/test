{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZXNSmq0XmBZo"
   },
   "source": [
    "# Part I. Basic Processing and Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c1N62U6DmOPH"
   },
   "source": [
    "## 1.1 Libraries\n",
    "For this task, we will mainly use methods from NLTK. You can replace them with SpaCy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "EZEEQcMRonz8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\satish.a.waghmare\\AppData\\Roaming\\nltk_data..\n",
      "[nltk_data]     .\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\satish.a.waghmare\\AppData\\Roaming\\nltk_data..\n",
      "[nltk_data]     .\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\satish.a.waghmare\\AppData\\Roaming\\nltk_data..\n",
      "[nltk_data]     .\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\satish.a.waghmare\\AppData\\Roaming\\nltk_data..\n",
      "[nltk_data]     .\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\satish.a.waghmare\\AppData\\Roaming\\nltk_data..\n",
      "[nltk_data]     .\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\satish.a.waghmare\\AppData\\Roaming\\nltk_data..\n",
      "[nltk_data]     .\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\satish.a.waghmare\\AppData\\Roaming\\nltk_data..\n",
      "[nltk_data]     .\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import string\n",
    "import numpy as np\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "nltk.download('stopwords')\n",
    "from wordcloud import WordCloud\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8xTVHpq4yJ4H"
   },
   "source": [
    "## 1.2 Text example from the Daily Mirror"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2_nYBKPVTHKO"
   },
   "outputs": [],
   "source": [
    "article = \"\"\"\n",
    "Brits are being warned to stay indoors as \"ferocious\" life-threatening 'day and night' temperatures are due to blast the UK for the first time in history.\n",
    "Death-Valley type temperatures of 42C during the day and 30C at night are threatening to double the UK summer average - prompting a Met Office first-ever red warning.\n",
    "The public are being urged \"to do as little as possible\" and try not to use public transport.\n",
    "Hospitals have cancelled appointments and many businesses have announced two day closures including restaurants, bars, zoos and wildlife centres - while millions will work from home.\n",
    "The chief executive of the College of Paramedics has warned that the \"ferocious heat\" the UK is predicted to experience over the next few days could result in people dying.\n",
    "She told Sky: \"We could see people who are vulnerable, young people, elderly people, people living with dementia who really do suffer.\n",
    "\"This is serious heat that could actually ultimately end in people's deaths because it is so ferocious. We are just not set up for that sort of heat in this country.\n",
    "\"This isn't like a lovely hot day where we can put a bit of sunscreen on, go out and enjoy a swim and a meal outside.\"\n",
    "Recent figures revealed how one in ten people waited 10 hours for an ambulance and the average waiting times for a heart attack was more than 50 minutes.\n",
    "\"It's difficult not to call it a crisis but it has been a stealth crisis,\" she added.\n",
    "Ministers are expected to hold a fourth Cobra meeting tomorrow after a national emergency was declared.\n",
    "Outgoing PM Boris Johnson didn't attend the last meeting on Saturday and went to Chequers instead.\n",
    "Labour's Angela Rayner blasted him for \"partying while Britain boils\" and described him as \"grotesque\".\n",
    "He snubbed the meeting as one large water company supplying Essex warned \"urgent action\" was needed while others said they were monitoring the situation.\n",
    "But despite universal pleas for the public to stay safe from health chiefs, not to travel and stay indoors, the Deputy Prime Minister Dominic Raab, decided to tell the public to \"enjoy the sunshine\" on Sky News.\n",
    "His comments were blasted on social media with one saying: \"Dominic Raab...brushing off the impending record breaking, earth burning temperatures and saying people should 'enjoy the sunshine'. Heaven help us all!\"\n",
    "Another added: \"I vote to put @DominicRaab in an office with no air con during the heatwave so he can enjoy the summertime\" and another said: \"He should try enjoying it in a white suit, respirator mask and goggles while resuscitating someone's Nan.\"\n",
    "BBC Weather presenter and meteorologist Simon King has pointed out that temperatures could reach 42°C in the East Midlands on Tuesday – tweeting that this level of heat \"just shouldn't happen.\"\n",
    "Affinity Water today said \"urgent action\" was needed in areas of Essex and Hertfordshire to conserve supplies, revealing demand had surged from 209 million gallons a day to 242 million gallons, due to the hot weather.\n",
    "It added it was \"working around the clock to maintain supplies\". Some areas of Buckinghamshire, Surrey and North London are also affected.\n",
    "Thames Water said they were monitoring the situation all the time but said if they did not see \"around or above average rainfall\" in the coming months it may result in water restrictions.\n",
    "Two thousand people in east Kent were left with no water or low pressure over the weekend.\n",
    "Southern Water said power issues on Friday evening caused a reservoir which serves the Broadstairs and St Peters areas to fail.\n",
    "Wildfires have already started amid fears for the safety of firefighters with six hectare grass fires in Hayes and a fire at Warsash nature reserve in Hampshire destroying 15,000 square metres of heathland.\n",
    "The rail network is about to go slow at a 20mph crawl, with some routes cancelled and gritters are out with sand to prevent the heat causing roads to \"liquefy\".\n",
    "The boss of Transport for London has urged Londoners to undertake only necessary travel on Monday and Tuesday.\n",
    "Andy Lord told LBC: \"We're advising all our customers to only travel if their journey is essential, to make sure that they stay hydrated and carry water with them if they do have to travel.\n",
    "\"Check before they travel because journey times will be extended.\n",
    "\"We will have reduced services across the TFL network because of the safety restrictions we need to put in place due to the heat.\"\n",
    "Avanti West Coast has urged passengers to travel only if it is \"absolutely necessary\" from Sunday until Tuesday.\n",
    "\"When the rail temperature gets above 50C, we have to reduce the speed to reduce the risk of damaging the track.\n",
    "In extreme cases, the rails can buckle, we need to avoid that and maintain the safety of the railway,\" they said.\n",
    "While London North Eastern Railway has urged people not to travel on Tuesday, warning that trains between London's King's Cross and the south of York and Leeds will not be running.\n",
    "Greater Manchester Police (GMP) have repeated warnings about cooling off in open water after a 16-year-old died while swimming with friends in a canal in Salford Quays.\n",
    "It is believed he was the fifth to die in the UK's waterways, with searches ongoing for a male at Ardsley Reservoir in West Yorkshire.\n",
    "Met Office meteorologist, Steve Keates, warned the public to \"do as little as possible\" in the extreme heat.\n",
    "\"Part of the reason the warnings are out is because it's not just day time but night time temperatures that are a concern,\" he told The Mirror.\n",
    "\"When you are going to bed on Monday night it could still be 30C or more at bed time or later evening and still high 20s into the early hours. It's horrible basically.\"\n",
    "He said hardest hit areas could be large urban areas such as London where the heat gets trapped in the city, taking longer to disperse.\n",
    "But he said a \"fascinating\" aspect is that higher up in the atmosphere it will get hotter meaning mountains and hilltops will be unusually \"quite a bit warmer\".\n",
    "\"Even at night it could still be exceptionally hot for the time of day and at places where you wouldn't necessarily expect it to be,\" he said.\n",
    "\"Our advice is to do as little as possible in temperatures like this.\"\n",
    "Scientist and climate specialist, John Grant, senior lecturer at Sheffield Hallam, warns that the heat could cause power cuts.\n",
    "He said: \"High temperatures on the road can damage the tarmac, liquify it.\n",
    "\"And the power stations require cooling and in extreme temperatures may reduce the water available or the water might not be cool enough to do the job.\n",
    "\"A few years ago some French nuclear power stations had to shut down because the rivers were too warm.\"\n",
    "The 40C heat set to grip Britain shows climate change \"really is a risk to health\", the Deputy Chief Medical Officer warned.\n",
    "England's DCMO Thomas Waite warned the \"extreme heat\" posed a threat to health and said: \"The scientific and medical communities are nearly in unanimous agreement that climate change really is a risk to health and we need to take carbon reduction and adaptations seriously.\n",
    "\"The thing that means for events like this is that the risk of this extreme heat is going up so we all need to sort of think about the steps we can take during extreme weather for how we avoid getting ourselves into trouble in the first place.\"\n",
    "NHS Confederation chairman Lord Adebowale said health staff \"are pretty stretched at the moment\".\n",
    "He added: \"Ambulances are operating at their peak, the waiting times for ambulances are now getting longer.\n",
    "\"We are going to be really, really pushed and it's not just the red warning, the heatwave - we are dealing with Covid, which is causing sickness in our ambulance crews.\"\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zyLulBR7yJ7g"
   },
   "source": [
    "## 1.3 Tokenization\n",
    "First, we split the raw text into sentences.\n",
    "\n",
    "Second, we split sentences into words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IWc1YBjQ-puD"
   },
   "outputs": [],
   "source": [
    "sentences = nltk.sent_tokenize(article)\n",
    "print(sentences)\n",
    "tokens = nltk.word_tokenize(sentences[0])\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qkzuV93w1zhN"
   },
   "source": [
    "## 1.4 Stopwords Removal\n",
    "\n",
    "Stop words are words that add no meaning to a sentence. These are modal verbs, articles, pronouns, etc.\n",
    "There are many lists of stop words. We will use the base one from NLTK.\n",
    "The decision to remove stop words or not depends on the task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ftGJGY6xLzFX"
   },
   "outputs": [],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "print(stopwords)\n",
    "\n",
    "stopwords_filtered = [w for w in tokens if not w.lower() in stopwords]\n",
    "stopwords_and_punct_filtered = [w for w in stopwords_filtered if w.isalnum()] # here we also remove punctuation marks\n",
    "print(tokens)\n",
    "print(stopwords_filtered)\n",
    "print(stopwords_and_punct_filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m_lxx6GA11A6"
   },
   "source": [
    "## 1.5 Wordcloud\n",
    "A wordcloud is a visual representation of words. Wordclouds are used to highlight popular words and phrases based on frequency. They provide you with quick and simple visual insights that can lead to deeper analysis.\n",
    "\n",
    "Word size depends on frequency. The largest words are the most common."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jQc848QF01d4"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,10))\n",
    "wc = WordCloud(background_color=\"black\", max_words=100, stopwords=stopwords, max_font_size= 40)\n",
    "wc.generate(article)\n",
    "plt.title(\"Heat\", fontsize=20)\n",
    "plt.imshow(wc.recolor( colormap= 'Pastel2' , random_state=17), alpha=0.98)\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WnaW-LcGx9JC"
   },
   "source": [
    "## 1.6 Stemming\n",
    "\n",
    "Stemming is the process of reducing inflected (or sometimes derived) words to their word stem, base or root form—generally a written word form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j1OJloCd_TLg"
   },
   "outputs": [],
   "source": [
    "stemmer = nltk.stem.PorterStemmer()\n",
    "stemmer.stem('extended')\n",
    "\n",
    "for tok in stopwords_and_punct_filtered:\n",
    "  print(stemmer.stem(tok))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SoQ1dG8dyIMM"
   },
   "source": [
    "## 1.7 Lemmatization\n",
    "Lemmatisation is the algorithmic process of determining the lemma of a word based on its intended meaning. Unlike stemming, lemmatisation depends on correctly identifying the intended part of speech and meaning of a word in a sentence, as well as within the larger context surrounding that sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Oo7wcQPsAAmm"
   },
   "outputs": [],
   "source": [
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "for tok in stopwords_and_punct_filtered:\n",
    "  print(lemmatizer.lemmatize(tok))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AoYrdZR6yIR6"
   },
   "source": [
    "## 1.8 POS Tagging\n",
    "POS tagging marks a word in the text as corresponding to a particular part of speech based on its definition and context.\n",
    "\n",
    "There are many different tagging systems. For this assignment, we will use the  [Penn Treebank](https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html) approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dZhVC7QQCfzh"
   },
   "outputs": [],
   "source": [
    "tags = nltk.pos_tag(nltk.word_tokenize(sentences[0]))\n",
    "tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7YTWbCnnyIUs"
   },
   "source": [
    "## 1.9 TDM, DGTM, Bag of Words\n",
    "\n",
    "A **document-term matrix** is a mathematical matrix that describes the frequency of terms that occur in a collection of documents. In a document-term matrix, rows correspond to documents in the collection and columns correspond to terms. It is also common to encounter the transpose, or **term-document matrix** where documents are the columns and terms are the rows.\n",
    "\n",
    "Terms are commonly single words separated by whitespace or punctuation on either side (a.k.a. unigrams). In such a case, this is also referred to as **bag of words** representation because the counts of individual words is retained, but not the order of the words in the document.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S4KG-R_N-M3x"
   },
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(stop_words=stopwords)\n",
    "\n",
    "DTM = pd.DataFrame(vectorizer.fit_transform(sentences).toarray(), columns=vectorizer.get_feature_names_out())\n",
    "print(DTM.shape)\n",
    "DTM.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OuQRJR1kCn9a"
   },
   "outputs": [],
   "source": [
    "TDM = DTM.T\n",
    "print(TDM.shape)\n",
    "TDM.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0Ol-8aeAyIXl"
   },
   "source": [
    "## 1.10 N-Grams\n",
    "An n-gram is a continuous sequence of n items from a given sample of text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cLnujCAjwHxl"
   },
   "outputs": [],
   "source": [
    "unigrams = nltk.ngrams(stopwords_and_punct_filtered, 1)\n",
    "[ ' '.join(grams) for grams in unigrams]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r75kLuCQEaga"
   },
   "outputs": [],
   "source": [
    "trigrams = nltk.ngrams(stopwords_and_punct_filtered, 3)\n",
    "[ ' '.join(grams) for grams in trigrams]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X6BSBq2NFQYs"
   },
   "outputs": [],
   "source": [
    "nltk.FreqDist(nltk.ngrams(stopwords_and_punct_filtered, 3)) # trigrams with frequencies; The counter can also be used instead of FreqDist."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5-8R-kuImAWq"
   },
   "source": [
    "## 1.11 TF-IDF\n",
    "The tf–idf is the product of two statistics, term frequency and inverse document frequency.\n",
    "\n",
    "Term frequency, tf(t,d), is the relative frequency of term t within document d,\n",
    "\n",
    "Inverse document frequency is a measure of how much information the word provides, i.e., if it is common or rare across all documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s1fIZwsetR3G"
   },
   "outputs": [],
   "source": [
    "tdidf_vectorizer = TfidfVectorizer(stop_words=stopwords)\n",
    "tdidf_vectorizer.fit_transform(sentences)\n",
    "\n",
    "df = pd.DataFrame(tdidf_vectorizer.fit_transform(sentences).toarray(), columns=tdidf_vectorizer.get_feature_names_out())\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K-jdK4Tvltvd"
   },
   "source": [
    "# Part II. Practice\n",
    "\n",
    "For this practice, we will use part of the author identification dataset from Kaggle.\n",
    "\n",
    "We need only the **text** column.\n",
    "\n",
    "People who like challenges can also try to solve a classification problem based on this. The original goal was to identify the author of sentences in the test set. The test set is not presented here, but you can split the data into train/validation/test and use any classification algorithm you know.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7DOd0j-1mVeY"
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('sample_data/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QhnRxRjsmYmT"
   },
   "outputs": [],
   "source": [
    "data[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pxv8ICuvmdae"
   },
   "outputs": [],
   "source": [
    "corpus = data[data.author==\"HPL\"][\"text\"].values\n",
    "hpl_string = ' '.join(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uQKCBbJ1re8c"
   },
   "source": [
    "Your tasks:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "osiMX501rGp6"
   },
   "source": [
    "Implement two functions that will split text into sentences and sentences into tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "takYn5Evrl-5"
   },
   "outputs": [],
   "source": [
    "def tokenize_sentences(txt: str) -> list:\n",
    "\n",
    "  sent = [] # PUT YOUR CODE HERE\n",
    "\n",
    "  return sent\n",
    "\n",
    "def tokenize_words(txt: str) -> list:\n",
    "\n",
    "  words = [] # PUT YOUR CODE HERE\n",
    "  \n",
    "  return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G_8ale2frmCj"
   },
   "outputs": [],
   "source": [
    "sentences = tokenize_sentences(hpl_string)\n",
    "words = tokenize_words(sentences[0])\n",
    "\n",
    "try:\n",
    "  assert words == ['It', 'never', 'once', 'occurred', 'to', 'me', 'that', 'the', 'fumbling', 'might', 'be', 'a', 'mere', 'mistake', '.']\n",
    "  print('passed')\n",
    "except AssertionError:\n",
    "  print('failed')\n",
    "  print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jAnuJX_YuP87"
   },
   "source": [
    "Implement two functions that will remove stop words and punctuation from the list of tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "77CYIctZuXuT"
   },
   "outputs": [],
   "source": [
    "def remove_stopwords(words: list) -> list:\n",
    "\n",
    "  words_cleaned = [] # PUT YOUR CODE HERE\n",
    "  \n",
    "  return words_cleaned\n",
    "\n",
    "def remove_punctuation(words: list) -> list:\n",
    "\n",
    "  words_cleaned = [] # PUT YOUR CODE HERE\n",
    "\n",
    "  return words_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "boFtLpr3t0sD"
   },
   "outputs": [],
   "source": [
    "words_wo_stopwords = remove_stopwords(words)\n",
    "words_wo_punct = remove_punctuation(words_wo_stopwords)\n",
    "\n",
    "try:\n",
    "  assert words_wo_punct == ['never', 'occurred', 'fumbling', 'might', 'mere', 'mistake']\n",
    "  print('passed')\n",
    "except AssertionError:\n",
    "  print('failed')\n",
    "  print(words_wo_punct)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1LAlLx1cwR6W"
   },
   "source": [
    "Implement two functions for stemming and lemmatization\n",
    "Цhat and when is best to use?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n3hCo5P6wgnM"
   },
   "outputs": [],
   "source": [
    "def words_stemming(words: list) -> list:\n",
    "\n",
    "  word_stem = [] # PUT YOUR CODE HERE\n",
    "  \n",
    "  return word_stem\n",
    "\n",
    "\n",
    "def words_lemma(words: list) -> list:\n",
    "\n",
    "  word_lemma = [] # PUT YOUR CODE HERE\n",
    "\n",
    "  return word_lemma   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ES2kY-hRwgqC"
   },
   "outputs": [],
   "source": [
    "word_stems = words_stemming(words_wo_punct)\n",
    "word_lemmas = words_lemma(words_wo_punct)\n",
    "\n",
    "try:\n",
    "  assert word_stems == ['never', 'occur', 'fumbl', 'might', 'mere', 'mistak']\n",
    "  assert word_lemmas == ['never', 'occurred', 'fumbling', 'might', 'mere', 'mistake']\n",
    "  print('passed')\n",
    "except AssertionError:\n",
    "  print('failed')\n",
    "  print(word_stems)\n",
    "  print(word_lemmas)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DkQX_JmdNN7K"
   },
   "source": [
    "Implement a function that will return a wordcloud and a dictionary with the most common words and their frequencies.\n",
    "\n",
    "<details>\n",
    "<summary>hints:</summary>\n",
    "\n",
    "Check the properties of the wordcloud object to see where the frequencies are stored.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AksqXiBEM_6g"
   },
   "outputs": [],
   "source": [
    "def make_wordclouds(txt: str, word_count: int) -> tuple: # output should be like (wordcloud object, {'a': 0.5, 'b': 0.7, ...})\n",
    "\n",
    "  out = [] # PUT YOUR CODE HERE\n",
    "\n",
    "  return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eW7Sg1sSM_9y"
   },
   "outputs": [],
   "source": [
    "top_5 = {'one': 1.0, 'thing': 0.9042056074766355, 'could': 0.8621495327102804, 'would': 0.8200934579439252, 'seemed': 0.6355140186915887}\n",
    "\n",
    "try:\n",
    "  assert make_wordclouds(hpl_string, 5)[1] == top_5\n",
    "  plt.figure(figsize=(7,4))\n",
    "  plt.imshow(make_wordclouds(hpl_string, 5)[0])\n",
    "  plt.axis('off')\n",
    "  print('passed')\n",
    "except AssertionError:\n",
    "  print('failed')\n",
    "  print(make_wordclouds(hpl_string, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HTQGCc3RPJlf"
   },
   "source": [
    "Implement a function that will return the first N most frequently occurring verbs in a text.\n",
    "\n",
    "First, use POS tagging to get only the verbs and then calculate the frequencies.\n",
    "\n",
    "<details>\n",
    "<summary>hints:</summary>\n",
    "\n",
    "Read this page to understand which tags are verbs [Penn Treebank](https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html)\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ekce7BqSNZdv"
   },
   "outputs": [],
   "source": [
    "def top_n_verbs(txt: str, n: int) -> list:\n",
    "\n",
    "  top_n = [] # PUT YOUR CODE HERE\n",
    "\n",
    "  return top_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kTE_tj1xNjAE"
   },
   "outputs": [],
   "source": [
    "top_7_verbs = ['was', 'had', 'were', 'be', 'have', 'been', 'is']\n",
    "try:\n",
    "  assert top_n_verbs(hpl_string, 7) == top_7_verbs\n",
    "  print('passed')\n",
    "except AssertionError:\n",
    "  print('failed')\n",
    "  print(top_n_verbs(hpl_string, 7))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hUzOnOB_PMWe"
   },
   "source": [
    "Implement a function that will return the first N most frequently occurring nouns in a text.\n",
    "\n",
    "The same as in the previous case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rV5YJyz8Nc9m"
   },
   "outputs": [],
   "source": [
    "def top_n_nouns(txt: str, n: int) -> list:\n",
    "\n",
    "  top_n = [] # PUT YOUR CODE HERE\n",
    "\n",
    "  return top_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dgTuuiZLNid9"
   },
   "outputs": [],
   "source": [
    "top_7_nouns = ['man', 'night', 'time', 'things', 'men', 'thing', 'house']\n",
    "try:\n",
    "  assert top_n_nouns(hpl_string, 7) == top_7_nouns\n",
    "  print('passed')\n",
    "except AssertionError:\n",
    "  print('failed')\n",
    "  print(top_n_nouns(hpl_string, 7))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k1FTTLIeN1DW"
   },
   "source": [
    "Feature engineering example\n",
    "\n",
    "Fill in all the empty rows using the first ones as examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2G1KDeWZ4y-c"
   },
   "outputs": [],
   "source": [
    "d = data.copy(deep=True)\n",
    "d[\"tokens\"] = d[\"text\"].apply(lambda x: tokenize_words(x))\n",
    "d[\"words\"] = d[\"tokens\"] .apply(lambda x: remove_punctuation(x))\n",
    "d[\"clean tokens\"] = d[\"words\"].apply(lambda x: remove_stopwords(x))\n",
    "d[\"lemmas\"] = d[\"clean tokens\"].apply(lambda x: words_lemma(x))\n",
    "d[\"num tokens\"] = # PUT HERE COUNT OF TOKENS\n",
    "d[\"num chars\"] = # PUT HERE COUNT OF CHARACTERS\n",
    "d[\"num lemmas\"] = # PUT HERE COUNT OF LEMMAS\n",
    "d[\"num stopwords and punct\"] = # PUT COUNT OF STOPWORDS AND PUNCTUATION MARKS\n",
    "d[\"num capitals\"] = # PUT HERE COUNT OF CAPITAL LETTERS\n",
    "d[\"avg word length\"] = # PUT HERE AVERAGE WORD LENGTH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H5PGxKTWyiYq"
   },
   "outputs": [],
   "source": [
    "d.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uDTlN3ZaM7pO"
   },
   "source": [
    "From that moment you can use this table as a template for a classification.\n",
    "\n",
    "Feel free to add another features like NER count, verbs/nouns/adjectives/... count, feminine/masculine pronouns count, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tar9gCBxymPa"
   },
   "outputs": [],
   "source": [
    "d.dropna(inplace=True)\n",
    "X = d[[\n",
    "    'num tokens',\n",
    "    'num chars',\n",
    "    'num lemmas',\n",
    "    'num stopwords and punctuation marks',\n",
    "    'num capitals',\n",
    "    'avg word length'\n",
    "    ]]\n",
    "\n",
    "# Label encode authors\n",
    "le = LabelEncoder()\n",
    "d['author_encoded'] = le.fit_transform(d['author'])\n",
    "\n",
    "y = d['author_encoded']\n",
    "\n",
    "# y = d[\"author\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mYdUId7W4RKN"
   },
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "params = {\n",
    "    'objective': 'multi:softmax',\n",
    "    'max_depth': 5,\n",
    "    'eta': 0.1,\n",
    "    'n_estimators': 100,\n",
    "    'num_class': 3\n",
    "    }         \n",
    "           \n",
    "xgb_clf = XGBClassifier(**params)\n",
    "\n",
    "xgb_clf.fit(X_train, y_train)\n",
    "y_pred = xgb_clf.predict(X_test)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0N2LjT491zWH"
   },
   "source": [
    "An alternative solution using the Bag of Words model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1S8QLlqrymSr"
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yIbxvq9Fytxu"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data.drop('author', axis=1), data['author'], train_size=0.7, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3rT8WclAyt3_"
   },
   "outputs": [],
   "source": [
    "bow = CountVectorizer()\n",
    "bow.fit(data['text'])\n",
    "X_train_bow = bow.transform(X_train['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Jiqq1CDlyt75"
   },
   "outputs": [],
   "source": [
    "nb_bow = MultinomialNB(alpha=1.0)\n",
    "nb_bow.fit(X_train_bow, y_train)\n",
    "y_pred = nb_bow.predict(X_train_bow)\n",
    "accuracy_score(y_train, y_pred) # training set accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LLjVm2JZznvO"
   },
   "outputs": [],
   "source": [
    "X_test_bow = bow.transform(X_test['text'])\n",
    "y_pred_bow = nb_bow.predict(X_test_bow)\n",
    "accuracy_score(y_test, y_pred_bow) # test set accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ONiRdRVKz-zg"
   },
   "source": [
    "Reproduce the same with Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E0EYstJN5h6m"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4FhDfV3Jzxrf"
   },
   "outputs": [],
   "source": [
    "# PUT YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OLWrE-tgzxuy"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "c1N62U6DmOPH",
    "8xTVHpq4yJ4H",
    "zyLulBR7yJ7g",
    "qkzuV93w1zhN",
    "m_lxx6GA11A6",
    "WnaW-LcGx9JC",
    "SoQ1dG8dyIMM",
    "AoYrdZR6yIR6",
    "7YTWbCnnyIUs",
    "0Ol-8aeAyIXl",
    "5-8R-kuImAWq"
   ],
   "name": "Text Preprocessing Pipeline.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
